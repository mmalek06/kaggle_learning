{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This went quite well - got 0.6995 on private score, 0.70083 on the public one. It probably could be tweaked as the neural network at the end does the difference. The other competitors mentioned scaling the target variable to [0-1] range and were saying it allowed them to go down a bit with their MSE, but given that this task got me tired as no other, for now I'm leaving the solution as is."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from pandas.io.parsers import TextFileReader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T14:49:47.645472Z",
     "end_time": "2023-04-23T14:49:52.181298Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def normalize(vals: np.ndarray):\n",
    "    min_val = np.min(vals)\n",
    "    max_val = np.max(vals)\n",
    "    normalized_values = (vals - min_val) / (max_val - min_val)\n",
    "\n",
    "    return normalized_values\n",
    "\n",
    "\n",
    "def restore(normalized_values: np.ndarray, vals: np.ndarray):\n",
    "    original_min = np.min(vals)\n",
    "    original_max = np.max(vals)\n",
    "    normalized_values = np.array(normalized_values)\n",
    "    restored_values = normalized_values * (original_max - original_min) + original_min\n",
    "\n",
    "    return restored_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T14:49:52.184201Z",
     "end_time": "2023-04-23T14:49:52.186320Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_full = pd.read_csv('data/train.csv')\n",
    "train_ids = train_full[['id']].to_numpy(dtype=int)\n",
    "xs = train_full.drop('id', axis=1).to_numpy()\n",
    "ys = train_full[['target']].to_numpy()\n",
    "test_full = pd.read_csv('data/test.csv')\n",
    "ids_test = test_full[['id']].to_numpy(dtype=int)\n",
    "test = test_full.drop('id', axis=1).to_numpy()\n",
    "split = int(xs.shape[0] * .8)\n",
    "X_train = xs[:split, :-1]\n",
    "y_train = ys[:split, -1]\n",
    "y_train_normalized = normalize(y_train)\n",
    "ids_train = train_ids[:split, 0]\n",
    "X_valid = xs[split:, :-1]\n",
    "y_valid = ys[split:, -1]\n",
    "y_valid_normalized = normalize(y_valid)\n",
    "ids_valid = train_ids[split:, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T14:49:52.189735Z",
     "end_time": "2023-04-23T14:49:53.470237Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class SwapNoise(keras.layers.Layer):\n",
    "    def __init__(self, ratio=0.15, col_to_apply=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "        self.col_to_apply = col_to_apply\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            noisy_inputs = tf.map_fn(lambda x: SwapNoise._add_swap_noise(x, ratio=self.ratio, col_to_apply=self.col_to_apply), inputs)\n",
    "            return noisy_inputs\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_swap_noise(X, ratio=.15, col_to_apply=None, return_mask=False):\n",
    "        if col_to_apply is None:\n",
    "            col_to_apply = []\n",
    "\n",
    "        shape = tf.shape(X)\n",
    "        obfuscation_mask = tf.cast(\n",
    "            tf.random.stateless_binomial(\n",
    "                shape=shape,\n",
    "                seed=(1, 2),\n",
    "                counts=1,\n",
    "                probs=tf.fill(shape, ratio)),\n",
    "            dtype=tf.float32)\n",
    "\n",
    "        if col_to_apply:\n",
    "            column_mask = np.zeros(X.shape, dtype=np.float32)\n",
    "            column_mask[col_to_apply] = 1\n",
    "            obfuscation_mask *= column_mask\n",
    "\n",
    "        shuffled_rows = tf.random.shuffle(tf.range(tf.shape(X)[0]))\n",
    "        obfuscated_X = tf.where(obfuscation_mask == 1, tf.gather(X, shuffled_rows), X)\n",
    "\n",
    "        if return_mask:\n",
    "            return obfuscated_X, obfuscation_mask\n",
    "\n",
    "        return obfuscated_X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T14:49:53.475181Z",
     "end_time": "2023-04-23T14:49:53.478318Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse._csr import csr_matrix\n",
    "\n",
    "from hyperopt import fmin, hp, tpe, STATUS_OK\n",
    "from hyperopt.pyll.base import scope\n",
    "from typing import Callable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T14:49:53.480486Z",
     "end_time": "2023-04-23T14:49:54.125633Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def rte_objective(search_space: dict) -> dict:\n",
    "    rte = RandomTreesEmbedding(**search_space)\n",
    "    ridge = Ridge(alpha=3000)\n",
    "\n",
    "    rte.fit(X_train)\n",
    "\n",
    "    X_train_transformed = rte.transform(X_train)\n",
    "\n",
    "    ridge.fit(X_train_transformed, y_train_normalized)\n",
    "\n",
    "    X_valid_transformed = rte.transform(X_valid)\n",
    "    y_pred = ridge.predict(X_valid_transformed)\n",
    "    accuracy = mean_squared_error(y_valid_normalized, y_pred)\n",
    "\n",
    "    return {'loss': accuracy, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def find_params(search_space: dict, get_objective: Callable, max_evals: int = 100) -> dict:\n",
    "    algorithm = tpe.suggest\n",
    "    best_params = fmin(\n",
    "        fn=get_objective,\n",
    "        space=search_space,\n",
    "        algo=algorithm,\n",
    "        max_evals=max_evals)\n",
    "\n",
    "    return best_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T14:49:54.129275Z",
     "end_time": "2023-04-23T14:49:54.132031Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [10:24:44<00:00, 249.90s/trial, best loss: 0.06101731938071909] \n"
     ]
    }
   ],
   "source": [
    "rte_params = find_params({\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 100, 2000, 50)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 5, 15, 1)),\n",
    "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 50, 400, 50)),\n",
    "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 25, 400, 25))}, rte_objective)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m----> 2\u001B[0m     rte_params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_depth\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[43mrte_params\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_depth\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      3\u001B[0m     rte_params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin_samples_leaf\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(rte_params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin_samples_leaf\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'rte_params' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mEOFError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNameError\u001B[39;00m:\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msaved_models\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrte_params.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fp:\n\u001B[1;32m---> 11\u001B[0m         rte_params \u001B[38;5;241m=\u001B[39m \u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mEOFError\u001B[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    rte_params['max_depth'] = int(rte_params['max_depth'])\n",
    "    rte_params['min_samples_leaf'] = int(rte_params['min_samples_leaf'])\n",
    "    rte_params['min_samples_split'] = int(rte_params['min_samples_split'])\n",
    "    rte_params['n_estimators'] = int(rte_params['n_estimators'])\n",
    "\n",
    "    with open(os.path.join('saved_params', 'rte_params.pkl'), 'wb') as fp:\n",
    "        pickle.dump(rte_params, fp)\n",
    "except NameError:\n",
    "    with open(os.path.join('saved_params', 'rte_params.pkl'), 'rb') as fp:\n",
    "        rte_params = pickle.load(fp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def xgb_objective(search_space: dict) -> dict:\n",
    "    regressor = xgb.XGBRegressor(**search_space)\n",
    "\n",
    "    regressor.fit(X_train, y_train_normalized)\n",
    "\n",
    "    y_pred = regressor.predict(X_valid)\n",
    "    accuracy = mean_squared_error(y_valid_normalized, y_pred)\n",
    "\n",
    "    return {'loss': accuracy, 'status': STATUS_OK}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T10:11:59.760376Z",
     "end_time": "2023-04-23T10:12:00.379344Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [20:51<00:00, 12.51s/trial, best loss: 0.018242681542883835]\n"
     ]
    }
   ],
   "source": [
    "xgb_params = find_params({\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 1, 15, 1)),\n",
    "    'gamma': hp.uniform ('gamma', 0, 1),\n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0, 1),\n",
    "    'min_child_weight' : hp.uniform('min_child_weight', 0, 10),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0, .3),\n",
    "    'random_state': 5,\n",
    "    'max_bin' : scope.int(hp.quniform('max_bin', 200, 650, 1))}, xgb_objective)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "try:\n",
    "    xgb_params['max_bin'] = int(xgb_params['max_bin'])\n",
    "    xgb_params['max_depth'] = int(xgb_params['max_depth'])\n",
    "\n",
    "    with open(os.path.join('saved_params', 'xgb_params.pkl'), 'wb') as fp:\n",
    "        pickle.dump(xgb_params, fp)\n",
    "except NameError:\n",
    "    with open(os.path.join('saved_params', 'xgb_params.pkl'), 'rb') as fp:\n",
    "        xgb_params = pickle.load(fp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(**xgb_params)\n",
    "\n",
    "regressor.fit(X_train, y_train_normalized)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Ridge(alpha=3000)",
      "text/html": "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=3000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=3000)</pre></div></div></div></div></div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte = RandomTreesEmbedding(**rte_params)\n",
    "ridge = Ridge(alpha=3000)\n",
    "\n",
    "rte.fit(X_train)\n",
    "\n",
    "X_train_transformed = rte.transform(X_train)\n",
    "X_valid_transformed = rte.transform(X_valid)\n",
    "\n",
    "ridge.fit(X_train_transformed, y_train_normalized)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def create_file(name: str, xs_trans: csr_matrix, xs_old: np.ndarray, ids: np.ndarray, ys: np.ndarray = None):\n",
    "    col_names = \\\n",
    "        ['id'] + \\\n",
    "        [f'cat{idx}' for idx in range(xs_trans.shape[1])] + \\\n",
    "        [f'cont{idx}' for idx in range(xs_old.shape[1])]\n",
    "\n",
    "    if ys is not None:\n",
    "        col_names += ['target']\n",
    "\n",
    "    with open(os.path.join('data', f'{name}_enhanced.csv'), 'w') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "\n",
    "        writer.writerow(col_names)\n",
    "\n",
    "        for i in range(0, xs_trans.shape[0]):\n",
    "            enhanced_repr = xs_trans.getrow(i).toarray()[0]\n",
    "            enhanced_repr = enhanced_repr.reshape(enhanced_repr.shape[0], 1)\n",
    "            old_features = xs_old[i, :]\n",
    "            old_features = old_features.reshape(old_features.shape[0], 1)\n",
    "            tall_repr = np.vstack([enhanced_repr, old_features])\n",
    "            wide_repr = tall_repr.reshape(1, tall_repr.shape[0]).tolist()[0]\n",
    "            user_id = [ids[i, 0]] if ids.ndim == 2 else [ids[i]]\n",
    "            full_row = user_id + wide_repr + ([ys[i]] if ys is not None else [])\n",
    "\n",
    "            writer.writerow(full_row)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I couldn't find a working way to create tfrecords directly from sparse matrix generated by the RTE, so I'm writing a temporary, huge file that is picked up by the next cells and transformed into multiple tfrecords."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "try:\n",
    "    create_file('train_normalized', X_train_transformed, X_train, ids_train, y_train_normalized)\n",
    "    create_file('valid_normalized', X_valid_transformed, X_valid, ids_valid, y_valid_normalized)\n",
    "except NameError:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def to_tf_records(filename: str, kind: str, categorical_cols: list[str], continuous_cols: list[str]):\n",
    "    def rows_to_example(rows: pd.DataFrame):\n",
    "        feature = { 'id': tf.train.Feature(int64_list=tf.train.Int64List(value=rows['id'].astype(int))) }\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            feature[col] = tf.train.Feature(int64_list=tf.train.Int64List(value=rows[col].astype(int)))\n",
    "        for col in continuous_cols:\n",
    "            feature[col] = tf.train.Feature(float_list=tf.train.FloatList(value=rows[col]))\n",
    "\n",
    "        feature['target'] = tf.train.Feature(float_list=tf.train.FloatList(value=rows['target']))\n",
    "\n",
    "        return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "    csv_file = os.path.join('data', f'{filename}.csv')\n",
    "    file_reader = pd.read_csv(csv_file, chunksize=1000, low_memory=False)\n",
    "    record_options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "    idx = 0\n",
    "\n",
    "    for frame in file_reader:\n",
    "        record_name = f'{filename}_{idx}.tfrecord'\n",
    "        output_file = os.path.join('data', 'records', kind, record_name)\n",
    "\n",
    "        with tf.io.TFRecordWriter(output_file, record_options) as writer:\n",
    "            example = rows_to_example(frame)\n",
    "\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "        print(f'Processed chunk {idx+1} into {output_file}')\n",
    "\n",
    "        idx += 1\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train_path = os.path.join('data', 'train_normalized_enhanced.csv')\n",
    "column_names = []\n",
    "\n",
    "# way faster than doing it with pandas\n",
    "with open(train_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "    for row in csv_reader:\n",
    "        column_names = row\n",
    "\n",
    "        break\n",
    "\n",
    "categorical_cols = list(filter(lambda n: n.startswith('cat'), column_names))\n",
    "continuous_cols = list(filter(lambda n: n.startswith('cont'), column_names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1 into data\\records\\train\\train_normalized_enhanced_0.tfrecord\n",
      "Processed chunk 1 into data\\records\\valid\\valid_normalized_enhanced_0.tfrecord\n"
     ]
    }
   ],
   "source": [
    "to_tf_records('train_normalized_enhanced', 'train', categorical_cols, continuous_cols)\n",
    "to_tf_records('valid_normalized_enhanced', 'valid', categorical_cols, continuous_cols)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "feature_description = {'id': tf.io.FixedLenFeature([], dtype=tf.int64)}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    feature_description[col] = tf.io.FixedLenFeature([], dtype=tf.int64)\n",
    "for col in continuous_cols:\n",
    "    feature_description[col] = tf.io.FixedLenFeature([], dtype=tf.float32)\n",
    "\n",
    "feature_description['target'] = tf.io.FixedLenFeature([], dtype=tf.float32)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def parse_example(record_bytes: list) -> tf.Tensor:\n",
    "    example = tf.io.parse_single_example(record_bytes, feature_description)\n",
    "    tensors = []\n",
    "\n",
    "    for feature_name in categorical_cols:\n",
    "        tensors.append(tf.cast(example[feature_name], tf.float32))\n",
    "    for feature_name in continuous_cols + ['target']:\n",
    "        tensors.append(example[feature_name])\n",
    "\n",
    "    data_vector = tf.stack(tensors)\n",
    "\n",
    "    return data_vector\n",
    "\n",
    "\n",
    "def get_dataset(files: list[str]) -> tf.data.Dataset:\n",
    "    return tf.data.TFRecordDataset(files, compression_type='GZIP')\\\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\\\n",
    "        .map(parse_example, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "        .batch(BATCH_SIZE) \\\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "# .shuffle(1000) \\"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large unrolled loop detected. Did you mean to use a TF loop? The following ops were created after iteration 50002: (<tf.Operation 'Cast_50000' type=Cast>,)\n",
      "See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/common_errors.md#warning-large-unrolled-loop-detected\n",
      "Location:\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n",
      "    self.io_loop.start()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n",
      "    res = shell.run_cell(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2945, in run_cell\n",
      "    result = self._run_cell(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3000, in _run_cell\n",
      "    return runner(coro)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3203, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3382, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\n",
      "  File \"C:\\Users\\mmale\\AppData\\Local\\Temp\\ipykernel_7412\\1564217771.py\", line 9, in <module>\n",
      "    train_dataset = get_dataset(train_files)\n",
      "\n",
      "  File \"C:\\Users\\mmale\\AppData\\Local\\Temp\\ipykernel_7412\\3774865490.py\", line 28, in get_dataset\n",
      "    return tf.data.TFRecordDataset(files, compression_type='GZIP')\\\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 2204, in map\n",
      "    return ParallelMapDataset(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 5441, in __init__\n",
      "    self._map_func = structured_function.StructuredFunctionWrapper(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py\", line 271, in __init__\n",
      "    self._function = fn_factory()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2610, in get_concrete_function\n",
      "    graph_function = self._get_concrete_function_garbage_collected(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2576, in _get_concrete_function_garbage_collected\n",
      "    graph_function, _ = self._maybe_define_function(args, kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2760, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2670, in _create_graph_function\n",
      "    func_graph_module.func_graph_from_py_func(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 1247, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py\", line 248, in wrapped_fn\n",
      "    ret = wrapper_helper(*args)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py\", line 177, in wrapper_helper\n",
      "    ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 915, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 963, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 785, in _initialize\n",
      "    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2523, in _get_concrete_function_internal_garbage_collected\n",
      "    graph_function, _ = self._maybe_define_function(args, kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2760, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2670, in _create_graph_function\n",
      "    func_graph_module.func_graph_from_py_func(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 1247, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 677, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 1222, in autograph_handler\n",
      "    return autograph.converted_call(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 439, in converted_call\n",
      "    result = converted_f(*effective_args, **kwargs)\n",
      "\n",
      "  File \"C:\\Users\\mmale\\AppData\\Local\\Temp\\__autograph_generated_filenf1zfiuf.py\", line 23, in tf__parse_example\n",
      "    ag__.for_stmt(ag__.ld(categorical_cols), None, loop_body, get_state, set_state, (), {'iterate_names': 'feature_name'})\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 463, in for_stmt\n",
      "    _py_for_stmt(iter_, extra_test, body, None, None)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 512, in _py_for_stmt\n",
      "    body(target)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 479, in protected_body\n",
      "    after_iteration()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 969, in after_iteration\n",
      "    did_warn = self._verify_inefficient_unroll()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 952, in _verify_inefficient_unroll\n",
      "    '', self.iterations, new_ops, '\\n'.join(traceback.format_stack()))\n",
      "\n",
      "WARNING: Large unrolled loop detected. Did you mean to use a TF loop? The following ops were created after iteration 50002: (<tf.Operation 'Cast_50000' type=Cast>,)\n",
      "See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/common_errors.md#warning-large-unrolled-loop-detected\n",
      "Location:\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 711, in start\n",
      "    self.io_loop.start()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 411, in do_execute\n",
      "    res = shell.run_cell(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 531, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2945, in run_cell\n",
      "    result = self._run_cell(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3000, in _run_cell\n",
      "    return runner(coro)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3203, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3382, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\n",
      "  File \"C:\\Users\\mmale\\AppData\\Local\\Temp\\ipykernel_7412\\1564217771.py\", line 9, in <module>\n",
      "    train_dataset = get_dataset(train_files)\n",
      "\n",
      "  File \"C:\\Users\\mmale\\AppData\\Local\\Temp\\ipykernel_7412\\3774865490.py\", line 28, in get_dataset\n",
      "    return tf.data.TFRecordDataset(files, compression_type='GZIP')\\\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 2204, in map\n",
      "    return ParallelMapDataset(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 5441, in __init__\n",
      "    self._map_func = structured_function.StructuredFunctionWrapper(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py\", line 271, in __init__\n",
      "    self._function = fn_factory()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2610, in get_concrete_function\n",
      "    graph_function = self._get_concrete_function_garbage_collected(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2576, in _get_concrete_function_garbage_collected\n",
      "    graph_function, _ = self._maybe_define_function(args, kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2760, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2670, in _create_graph_function\n",
      "    func_graph_module.func_graph_from_py_func(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 1247, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py\", line 248, in wrapped_fn\n",
      "    ret = wrapper_helper(*args)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py\", line 177, in wrapper_helper\n",
      "    ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 915, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 963, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 785, in _initialize\n",
      "    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2523, in _get_concrete_function_internal_garbage_collected\n",
      "    graph_function, _ = self._maybe_define_function(args, kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2760, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2670, in _create_graph_function\n",
      "    func_graph_module.func_graph_from_py_func(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 1247, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 677, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 1222, in autograph_handler\n",
      "    return autograph.converted_call(\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 439, in converted_call\n",
      "    result = converted_f(*effective_args, **kwargs)\n",
      "\n",
      "  File \"C:\\Users\\mmale\\AppData\\Local\\Temp\\__autograph_generated_filenf1zfiuf.py\", line 23, in tf__parse_example\n",
      "    ag__.for_stmt(ag__.ld(categorical_cols), None, loop_body, get_state, set_state, (), {'iterate_names': 'feature_name'})\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 463, in for_stmt\n",
      "    _py_for_stmt(iter_, extra_test, body, None, None)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 512, in _py_for_stmt\n",
      "    body(target)\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 479, in protected_body\n",
      "    after_iteration()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 969, in after_iteration\n",
      "    did_warn = self._verify_inefficient_unroll()\n",
      "\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py\", line 952, in _verify_inefficient_unroll\n",
      "    '', self.iterations, new_ops, '\\n'.join(traceback.format_stack()))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_records_folder = os.path.join('data', 'records', 'train')\n",
    "valid_records_folder = os.path.join('data', 'records', 'valid')\n",
    "train_files = [\n",
    "    os.path.join(train_records_folder, name)\n",
    "    for name in os.listdir(train_records_folder) if name.endswith('.tfrecord')]\n",
    "valid_files = [\n",
    "    os.path.join(valid_records_folder, name)\n",
    "    for name in os.listdir(valid_records_folder) if name.endswith('.tfrecord')]\n",
    "train_dataset = get_dataset(train_files)\n",
    "valid_dataset = get_dataset(valid_files)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cat_cols_len = len(categorical_cols)\n",
    "regressor_enhanced = keras.Sequential([\n",
    "    SwapNoise(ratio=.1, col_to_apply=[cat_cols_len + idx for idx in range(len(continuous_cols))]),\n",
    "    keras.layers.Dense(512),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(.5),\n",
    "    keras.layers.Dense(512),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(.5),\n",
    "    keras.layers.Dense(512),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(.5),\n",
    "    keras.layers.Dense(512),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(.3),\n",
    "    keras.layers.Dense(1),\n",
    "    keras.layers.PReLU()\n",
    "])\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=30,\n",
    "                                               min_delta=1e-6)\n",
    "regressor_enhanced.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history_enhanced = regressor_enhanced.fit(\n",
    "    train_dataset, epochs=1,\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath='saved_models/dnn2_regressor_after_dae_enhanced{epoch}',\n",
    "            save_best_only=True)],\n",
    "    validation_data=valid_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enhanced_regressor = keras.models.load_model(os.path.join('saved_models', 'dnn2_regressor_after_dae_enhanced6'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_transformed = rte.transform(test)\n",
    "ridge_predictions = ridge.predict(X_test_transformed)\n",
    "xgb_predictions = regressor.predict(test)\n",
    "\n",
    "create_file('test', X_test_transformed, test, ids_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow import Tensor\n",
    "\n",
    "\n",
    "def parse_test_batch(tf_string: Tensor):\n",
    "    zf = tf.zeros(shape=(1,), dtype=tf.float32)\n",
    "    defaults = [zf] * (14628 + 14 + 2)\n",
    "    data = tf.io.decode_csv(tf_string, defaults)\n",
    "    features = data[1:]\n",
    "    features = tf.stack(features, axis=-1)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_test_batched_dataset(batch_size: int, data_path: str) -> tf.data.Dataset:\n",
    "    return tf.data.TextLineDataset([data_path]) \\\n",
    "        .skip(1) \\\n",
    "        .batch(batch_size) \\\n",
    "        .map(parse_test_batch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dataset = get_test_batched_dataset(64, os.path.join('data', 'test_enhanced.csv'))\n",
    "nn_predictions = enhanced_regressor.predict(test_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vote():\n",
    "    reshaped_ridge_predictions = ridge_predictions.reshape((ridge_predictions.shape[0], 1))\n",
    "    reshaped_xgb_predictions = xgb_predictions.reshape((xgb_predictions.shape[0], 1))\n",
    "    avg = (reshaped_ridge_predictions + reshaped_xgb_predictions) / 2\n",
    "    avg_restored = restore(avg, y_train)\n",
    "\n",
    "    pd \\\n",
    "        .DataFrame(np.column_stack([pd.read_csv('data/test_enhanced.csv')[['id']], avg_restored]), columns=['id', 'target']) \\\n",
    "        .astype({'id': int}) \\\n",
    "        .to_csv(os.path.join('submissions', 'ensemble3.csv'), index=False)\n",
    "\n",
    "\n",
    "vote()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
